{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte-Pair Encoding (BPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Let's build initial vocab from the corpus (list of sentences).\n",
    "def build_vocab(corpus):\n",
    "    \"\"\"\n",
    "    Build the initial vocabulary from the corpus.\n",
    "    Each word is split into characters with and end-of-word marker </w> added.\n",
    "    \"\"\"\n",
    "    vocab = collections.Counter()\n",
    "    for sentence in corpus:\n",
    "        words = sentence.strip().split() \n",
    "        for word in words:\n",
    "            symbols = tuple(word) + ('</w>',)\n",
    "            vocab[symbols] += 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: A function to get a symbol pair frequency given a vocab\n",
    "def get_pair_frequencies(vocab):\n",
    "    \"\"\"\n",
    "    Count frequencies of adjacent symbol pairs in the vocabulary.\n",
    "    \"\"\"\n",
    "    pair_freqs = collections.Counter()\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pair = (symbols[i], symbols[i+1])\n",
    "            pair_freqs[pair] += freq\n",
    "    return pair_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Merge the Most Frequent Pair\n",
    "def merge_vocab(pair, vocab):\n",
    "    \"\"\"\n",
    "    Merge the most frequent pair in the vocabulary.\n",
    "    \"\"\"\n",
    "    merged_vocab = {}\n",
    "    bigram = ' '.join(pair)\n",
    "    pattern = re.compile(r'(?<!\\S)' + re.escape(bigram) + r'(?!\\S)')\n",
    "    for word, freq in vocab.items():\n",
    "        word_str = ' '.join(word)\n",
    "        # Replace the pair with merged symbol\n",
    "        word_str_new = pattern.sub(''.join(pair), word_str)\n",
    "        # Convert back to tuple of symbols\n",
    "        symbols = tuple(word_str_new.split())\n",
    "        merged_vocab[symbols] = freq\n",
    "    return merged_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Using functions until now, build the learner\n",
    "\n",
    "def learn_bpe(corpus, num_merges):\n",
    "    \"\"\"\n",
    "    Learn BPE merge operations from the corpus.\n",
    "    \"\"\"\n",
    "    vocab = build_vocab(corpus)\n",
    "    bpe_codes = []\n",
    "\n",
    "    for i in range(num_merges):\n",
    "        pair_freqs = get_pair_frequencies(vocab)\n",
    "        if not pair_freqs:\n",
    "            break\n",
    "        # get most frequent pair\n",
    "        most_frequent = max(pair_freqs, key=pair_freqs.get)\n",
    "        vocab = merge_vocab(most_frequent, vocab)\n",
    "        bpe_codes.append(most_frequent)\n",
    "\n",
    "    return bpe_codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Let's make the tokenizer\n",
    "\n",
    "def get_pairs(word):\n",
    "    \"\"\"\n",
    "    Get all adjacent symbol pairs in the word.\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    for i in range(len(word) - 1):\n",
    "        pairs.add((word[i], word[i+1]))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(word, bpe_codes):\n",
    "    \"\"\"\n",
    "    Tokenize a word using a learned BPE codes.\n",
    "    \"\"\"\n",
    "    word = tuple(word) + ('</w>',)\n",
    "    pairs = get_pairs(word)\n",
    "    bpe_codes_dict = {\n",
    "        pair: idx for idx, pair in enumerate(bpe_codes)\n",
    "    }\n",
    "\n",
    "    while True:\n",
    "        min_pair = None\n",
    "        min_rank = float('inf')\n",
    "\n",
    "        # find the pair with lowest rank (earliest merge)\n",
    "        for pair in pairs:\n",
    "            rank = bpe_codes_dict.get(pair, float('inf'))\n",
    "            if rank < min_rank:\n",
    "                min_rank = rank\n",
    "                min_pair = pair\n",
    "        \n",
    "        if min_pair is None or min_pair not in bpe_codes_dict:\n",
    "            break\n",
    "\n",
    "        # Merge the best pair\n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            j = i\n",
    "            while j < len(word) - 1 and (word[j], word[j+1]) != min_pair:\n",
    "                    j += 1\n",
    "            new_word.extend(word[i:j])\n",
    "            if j < len(word) - 1:\n",
    "                 new_word.append(''.join(min_pair))\n",
    "                 i = j + 2\n",
    "            else:\n",
    "                 new_word.extend(word[j:])\n",
    "                 break\n",
    "        word = tuple(new_word)\n",
    "        if len(word) == 1:\n",
    "             break\n",
    "        else:\n",
    "             pairs = get_pairs(word)\n",
    "    \n",
    "    if word[-1] == '</w>':\n",
    "         word = word[:-1]\n",
    "    \n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text, bpe_codes):\n",
    "    \"\"\"\n",
    "    Tokenize each word in the text using the learned BPE codes.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    for word in text.strip().split():\n",
    "        tokenized_word = tokenize(word, bpe_codes)\n",
    "        tokens.append(tokenized_word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example runs\n",
    "\n",
    "training_corpus = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Artificial intelligence is transforming industries worldwide.\",\n",
    "    \"She enjoys reading books by the lake during weekends.\",\n",
    "    \"The train to Tokyo departs at 10:30 AM sharp.\",\n",
    "    \"A healthy diet includes fruits, vegetables, and plenty of water.\",\n",
    "    \"Learning new languages can open doors to exciting opportunities.\",\n",
    "    \"The stars were shining brightly in the clear night sky.\",\n",
    "    \"Climate change is one of the biggest challenges of our time.\",\n",
    "    \"He plays the guitar beautifully, especially classical pieces.\",\n",
    "    \"The software update introduced several new features.\",\n",
    "    \"An elephant is the largest land animal on Earth.\",\n",
    "    \"They celebrated her birthday with a surprise party.\",\n",
    "    \"Technology is advancing at an unprecedented pace.\",\n",
    "    \"The team worked late to finish the project before the deadline.\",\n",
    "    \"Many rural areas still lack access to clean drinking water.\",\n",
    "    \"The museum was filled with ancient artifacts and rare paintings.\",\n",
    "    \"Good communication skills are essential for any career.\",\n",
    "]\n",
    "\n",
    "test_corpus = [\n",
    "    \"The app provides real-time weather updates and forecasts.\",\n",
    "    \"Hiking in the mountains is a great way to unwind and relax.\",\n",
    "    \"He enjoys experimenting with new recipes in the kitchen.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus Statistics\n",
      "1. Total Words : 151\n",
      "2. Unique Words : 126\n"
     ]
    }
   ],
   "source": [
    "# learn about the data to decide num_merges\n",
    "\n",
    "unique_words = set()\n",
    "total_words = 0\n",
    "for sentence in training_corpus:\n",
    "    words = sentence.split()\n",
    "    total_words += len(words)\n",
    "    for word in words:\n",
    "        unique_words.add(word)\n",
    "\n",
    "print(\"Corpus Statistics\")\n",
    "print(f\"1. Total Words : {total_words}\")\n",
    "print(f\"2. Unique Words : {len(unique_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Learned BPE Codes: 25\n",
      "[('e', '</w>'), ('s', '</w>'), ('i', 'n'), ('.', '</w>'), ('h', 'e</w>'), ('a', 'n'), ('a', 'r'), ('y', '</w>'), ('a', 't'), ('a', 'l'), ('e', 'n'), ('in', 'g'), ('e', 's'), ('d', '</w>'), ('t', 'he</w>'), ('ing', '</w>'), ('e', 'r'), ('t', 'i'), ('o', 'r'), ('t', '</w>'), ('T', 'he</w>'), ('al', '</w>'), ('l', 'a'), ('l', 'l'), ('e', 'a')]\n"
     ]
    }
   ],
   "source": [
    "num_merges = 25\n",
    "bpe_codes = learn_bpe(training_corpus, num_merges)\n",
    "print(f\"Number of Learned BPE Codes: {len(bpe_codes)}\")\n",
    "print(bpe_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing #1th sentence\n",
      "Original:  The app provides real-time weather updates and forecasts.\n",
      "Tokens:  [('The</w>',), ('a', 'p', 'p'), ('p', 'r', 'o', 'v', 'i', 'd', 'e', 's</w>'), ('r', 'e', 'al', '-', 'ti', 'm', 'e</w>'), ('w', 'e', 'at', 'h', 'er'), ('u', 'p', 'd', 'at', 'e', 's</w>'), ('an', 'd</w>'), ('f', 'or', 'e', 'c', 'a', 's', 't', 's', '.</w>')]\n",
      "\n",
      "\n",
      "\n",
      "Tokenizing #2th sentence\n",
      "Original:  Hiking in the mountains is a great way to unwind and relax.\n",
      "Tokens:  [('H', 'i', 'k', 'ing</w>'), ('in',), ('the</w>',), ('m', 'o', 'u', 'n', 't', 'a', 'in', 's</w>'), ('i', 's</w>'), ('a',), ('g', 'r', 'e', 'at'), ('w', 'a', 'y</w>'), ('t', 'o'), ('u', 'n', 'w', 'in', 'd</w>'), ('an', 'd</w>'), ('r', 'e', 'la', 'x', '.</w>')]\n",
      "\n",
      "\n",
      "\n",
      "Tokenizing #3th sentence\n",
      "Original:  He enjoys experimenting with new recipes in the kitchen.\n",
      "Tokens:  [('H', 'e</w>'), ('en', 'j', 'o', 'y', 's</w>'), ('e', 'x', 'p', 'er', 'i', 'm', 'en', 't', 'ing</w>'), ('w', 'i', 't', 'h'), ('n', 'e', 'w'), ('r', 'e', 'c', 'i', 'p', 'e', 's</w>'), ('in',), ('the</w>',), ('k', 'i', 't', 'c', 'h', 'en', '.</w>')]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tokenize the test corpus\n",
    "\n",
    "for i, sentence in enumerate(test_corpus):\n",
    "    print(f\"Tokenizing #{i+1}th sentence\")\n",
    "    tokens = encode_text(sentence, bpe_codes)\n",
    "    print(\"Original: \", sentence)\n",
    "    print(\"Tokens: \", tokens)\n",
    "\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
