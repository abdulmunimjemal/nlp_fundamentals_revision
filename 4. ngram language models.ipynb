{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram Language Modeling \n",
    "\n",
    "In this notebook, we will implement a stastical method called \"ngram\" to model a language.\n",
    "Regardless of its limitations, it's still a good place to start learning language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~cipy (C:\\Users\\Abdulmunim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\Abdulmunim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~cipy (C:\\Users\\Abdulmunim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\Abdulmunim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~cipy (C:\\Users\\Abdulmunim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\Abdulmunim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install requests nltk --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's setupa  function to fetch us a corpus of text\n",
    "\n",
    "def fetch_shakespeare():\n",
    "    import requests\n",
    "    url = \"https://www.gutenberg.org/files/100/100-0.txt\"\n",
    "    response = requests.get(url)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = fetch_shakespeare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Naive Statistics ---\n",
      "Length of the corpus:  5555456\n",
      "Number of words in the corpus:  963478\n",
      "Number of unique words in the corpus:  71167\n",
      "Number of characters in the corpus:  101\n",
      "Number of lines in the corpus:  196018\n",
      "Number of paragraphs in the corpus:  1\n",
      "Number of sentences in the corpus:  91163\n"
     ]
    }
   ],
   "source": [
    "# Initial EDA and Statistics on the corpus\n",
    "print(\"--- Naive Statistics ---\")\n",
    "print(\"Length of the corpus: \", len(corpus))\n",
    "print(\"Number of words in the corpus: \", len(corpus.split()))\n",
    "print(\"Number of unique words in the corpus: \", len(set(corpus.split())))\n",
    "print(\"Number of characters in the corpus: \", len(set(corpus)))\n",
    "print(\"Number of lines in the corpus: \", len(corpus.split(\"\\n\")))\n",
    "print(\"Number of paragraphs in the corpus: \", len(corpus.split(\"\\n\\n\")))\n",
    "print(\"Number of sentences in the corpus: \", len(corpus.split(\".\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Abdulmunim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing the corpus\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    import re\n",
    "    # Remove Project Gutenberg's header and footer\n",
    "    text = text[50:-55]\n",
    "    # lowercase the text\n",
    "    text = text.lower()\n",
    "    # remove non-alphabetic characters and extra spaces\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Tokenize using NLTK\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = preprocess_text(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "963637\n",
      "['their', 'queen', 'means', 'to', 'immure', 'herself', 'and', 'not', 'be', 'seen']\n",
      "['the', 'complete', 'works', 'of', 'william', 'shakespeare', 'by', 'william', 'shakespeare', 'contents']\n",
      "Average length of a token:  4.210134106515213\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))\n",
    "print(tokens[-10:])\n",
    "print(tokens[:10])\n",
    "# average length\n",
    "print(\"Average length of a token: \", sum([len(token) for token in tokens])/len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate n-grams from list of tokens\n",
    "\n",
    "def generate_ngrams(tokens, n):\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens)-n+1):\n",
    "        ngrams.append(tuple(tokens[i:i+n]))\n",
    "    return ngrams\n",
    "\n",
    "n = 6\n",
    "ngrams = generate_ngrams(tokens, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "963632"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the ngram model (Frequency distributions and conditional probablity tables for the n-grams)\n",
    "\n",
    "def build_ngram_model(tokens, n, ngrams=None):\n",
    "    from collections import Counter, defaultdict\n",
    "    model = defaultdict(Counter)\n",
    "    if ngrams is None:\n",
    "        ngrams = generate_ngrams(tokens, n)\n",
    "    for ngram in ngrams:\n",
    "        prefix = ngram[:-1]\n",
    "        suffix = ngram[-1]\n",
    "        model[prefix][suffix] += 1\n",
    "    return model\n",
    "\n",
    "ngram_model = build_ngram_model(tokens, n=4, ngrams=ngrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's normalize the probabilities\n",
    "\n",
    "def normalize_model(model):\n",
    "    normalized_model = {}\n",
    "    for prefix, suffixes in model.items():\n",
    "        total = float(sum(suffixes.values()))\n",
    "        normalized_suffixes = {}\n",
    "        for suffix, count in suffixes.items():\n",
    "            normalized_suffixes[suffix] = count/total\n",
    "        normalized_model[prefix] = normalized_suffixes\n",
    "    return normalized_model\n",
    "\n",
    "normalized_model = normalize_model(ngram_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "953106"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(normalized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's attempt to generate some text using the n-gram model\n",
    "\n",
    "def generate_text(model, n, num_words, start_prefix=None):\n",
    "    import random\n",
    "    if start_prefix is None:\n",
    "        start_prefix = random.choice(list(model.keys()))\n",
    "    else:\n",
    "        start_prefix = tuple(start_prefix.lower().split())\n",
    "        if len(start_prefix) != n - 1:\n",
    "            raise ValueError(f\"Start prefix must be {n - 1} words long.\")\n",
    "\n",
    "    generated = list(start_prefix)\n",
    "\n",
    "    for _ in range(num_words):\n",
    "        prefix = tuple(generated[-(n-1):])\n",
    "        suffixes = model.get(prefix, None)\n",
    "        if not suffixes:\n",
    "            break  # Cannot continue if there is no entry for this prefix\n",
    "        suffix = random.choices(list(suffixes.keys()), weights=suffixes.values())[0]\n",
    "        generated.append(suffix)\n",
    "\n",
    "    return ' '.join(generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = generate_text(normalized_model, 6, 10, \"by my hand i swear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "by my hand i swear and my fathers soul the work ish ill done it\n"
     ]
    }
   ],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
